{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f4ac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sympy.parsing.latex import parse_latex\n",
    "from sympy import Basic\n",
    "import sympy\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a160b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"zwhe99/DeepMath-103K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7380e52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aa73d7",
   "metadata": {},
   "source": [
    "# Preliminary tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4b8150",
   "metadata": {},
   "source": [
    "## Briefly describe the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a49347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28110471",
   "metadata": {},
   "source": [
    "## Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7226ad20",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- [] Questions have different latex sytanx (`[`, `(`, `$`)\n",
    "- [] How do we tokenize the latex formula?\n",
    "  - [] `hybrid_tokenize` with word lenght tokens\n",
    "  - [] BERT\n",
    "  - [] https://github.com/google/sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa12b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0a6f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5adf135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all the questions\n",
    "train_ds_qs = train_ds['question'][:NUM_SAMPLES]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e692c9eb",
   "metadata": {},
   "source": [
    "### Initial investigation on sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83871f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correctly parsed 7 inputs over 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_latex_bracket_content(text):\n",
    "    patterns = [\n",
    "        r'\\\\\\[(.*?)\\\\\\]',  # \\[ ... \\]\n",
    "        r'\\\\\\((.*?)\\\\\\)',  # \\( ... \\)\n",
    "        r'\\$(.*?)\\$',       # $ ... $\n",
    "    ]\n",
    "    # Combine patterns into one\n",
    "    combined_pattern = '|'.join(patterns)\n",
    "    #matches = [m for m in re.findall(combined_pattern, text, flags=re.DOTALL)]\n",
    "    #formulas = [next(filter(None, tup)) for tup in matches]\n",
    "    matches = [m for m in re.findall(combined_pattern, text, flags=re.DOTALL)]\n",
    "    formulas = []\n",
    "    for tup in matches:\n",
    "        non_empty = list(filter(None, tup))\n",
    "        if non_empty:\n",
    "            formulas.append(non_empty[0])\n",
    "    # Use a counter to number the placeholders\n",
    "    def replacer(match, counter=[1]):\n",
    "        placeholder = \"{\" + str(counter[0]-1) + \"}\"\n",
    "        counter[0] += 1\n",
    "        return placeholder\n",
    "    new_text = re.sub(combined_pattern, replacer, text)\n",
    "    return new_text, formulas\n",
    "\n",
    "parsed_indices = []\n",
    "parsed_text = []\n",
    "parsed_formulas = []\n",
    "for i in range(len(train_ds_qs)):\n",
    "    d = train_ds_qs[i]\n",
    "    #print(\"original input:\", d)\n",
    "    text, formulas = extract_latex_bracket_content(d)\n",
    "    #print(\"parsed text: \", text, \"\\nequations: \", formulas)\n",
    "    formulas_sympy = []\n",
    "    num_parsed = 0\n",
    "    for expr in formulas:\n",
    "        try:\n",
    "            parsed = parse_latex(expr)\n",
    "            formulas_sympy.append(parsed)\n",
    "            num_parsed += 1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            #print(\"Could not parse:\", e)\n",
    "            #print(parsed)\n",
    "    if num_parsed == len(formulas):\n",
    "        parsed_indices.append(i)\n",
    "        parsed_text.append(text)\n",
    "        parsed_formulas.append(formulas_sympy)\n",
    "print(\"correctly parsed\", len(parsed_indices), \"inputs over\", len(train_ds_qs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f65762",
   "metadata": {},
   "source": [
    "### Hybrid tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fa8dfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sympy_tree(expr, indent=0):\n",
    "    print('  ' * indent + str(expr.func))\n",
    "    for arg in expr.args:\n",
    "        print_sympy_tree(arg, indent + 1)\n",
    "\n",
    "# --- Parse LaTeX into sympy then walk the expression tree ---\n",
    "def flatten_sympy_expr(expr: Basic):\n",
    "    tokens = []\n",
    "\n",
    "    def walk(node):\n",
    "        if isinstance(node, Basic):\n",
    "            args_len = len(node.args)\n",
    "            args = node.args\n",
    "            # If only 1 we build the output as operator + walk(operand)\n",
    "            if args_len == 1:\n",
    "                if isinstance(node, (sympy.Symbol, sympy.Integer, sympy.Rational, sympy.Float)):\n",
    "                    tokens.append(str(node))\n",
    "                else:\n",
    "                    tokens.append(node.func.__name__)\n",
    "                walk(args[0])\n",
    "            # If only 2 we build the output with inorder visit walk(operand0) + operator + walk(operand1)\n",
    "            elif args_len == 2:\n",
    "                walk(args[0])\n",
    "                if isinstance(node, (sympy.Symbol, sympy.Integer, sympy.Rational, sympy.Float)):\n",
    "                    tokens.append(str(node))\n",
    "                else:\n",
    "                    tokens.append(node.func.__name__)\n",
    "                walk(args[1])\n",
    "            # If no operands are found we just add self\n",
    "            elif args_len == 0:\n",
    "                if isinstance(node, (sympy.Symbol, sympy.Integer, sympy.Rational, sympy.Float)):\n",
    "                    tokens.append(str(node))\n",
    "                else:\n",
    "                    tokens.append(node.func.__name__)\n",
    "            # If there are multiple operands (> 2) we add them all after the operator\n",
    "            else:\n",
    "                if isinstance(node, (sympy.Limit, sympy.Integral)):\n",
    "                    args = reversed(args)\n",
    "                if isinstance(node, (sympy.Symbol, sympy.Integer, sympy.Rational, sympy.Float)):\n",
    "                    tokens.append(str(node))\n",
    "                else:\n",
    "                    tokens.append(node.func.__name__)\n",
    "                for arg in args:\n",
    "                    walk(arg)\n",
    "        else:\n",
    "            raise RuntimeError(\"not sympy Basic object\")\n",
    "    \n",
    "    walk(expr)\n",
    "    return tokens\n",
    "\n",
    "# --- Main tokenization function ---\n",
    "def hybrid_tokenize(text):\n",
    "    math_pattern = r'(\\$\\$.*?\\$\\$|\\$.*?\\$|\\\\\\[.*?\\\\\\]|\\\\\\(.*?\\\\\\))'\n",
    "    parts = re.split(math_pattern, text, flags=re.DOTALL)\n",
    "\n",
    "    final_tokens = []\n",
    "    is_erorr = False\n",
    "    for part in parts:\n",
    "        if re.match(math_pattern, part, flags=re.DOTALL):\n",
    "            # Clean math delimiters\n",
    "            clean = re.sub(r'^(\\$+|\\\\\\[|\\\\\\(|\\\\])|(\\$+|\\\\\\]|\\\\\\))$', '', part.strip())\n",
    "            try:\n",
    "                parsed = parse_latex(clean)\n",
    "                math_tokens = flatten_sympy_expr(parsed)\n",
    "                final_tokens.extend(math_tokens)\n",
    "            except Exception as e:\n",
    "                is_erorr = True\n",
    "                final_tokens.extend(clean.split(' '))\n",
    "        else:\n",
    "            final_tokens.extend(re.findall(r'\\b\\w+\\b', part))\n",
    "    return is_erorr, final_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a3a86d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correctly parsed 7 inputs over 10\n",
      "not parsed 3 inputs over 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parsed_inputs = []\n",
    "not_parsed_inputs = []\n",
    "for d in train_ds_qs:\n",
    "    error, tokens = hybrid_tokenize(d)\n",
    "    if not error:\n",
    "        parsed_inputs.append(tokens)\n",
    "    else:\n",
    "        not_parsed_inputs.append(tokens)\n",
    "\n",
    "\n",
    "print(\"correctly parsed\", len(parsed_inputs), \"inputs over\", len(train_ds_qs))\n",
    "print(\"not parsed\", len(not_parsed_inputs), \"inputs over\", len(train_ds_qs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99441bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Find',\n",
       "  'the',\n",
       "  'length',\n",
       "  'of',\n",
       "  'the',\n",
       "  'polar',\n",
       "  'curve',\n",
       "  'given',\n",
       "  'by',\n",
       "  'r',\n",
       "  'Equality',\n",
       "  '1',\n",
       "  'Add',\n",
       "  'cos',\n",
       "  '2',\n",
       "  'Mul',\n",
       "  'theta',\n",
       "  'Pow',\n",
       "  '1/2',\n",
       "  'for',\n",
       "  '',\n",
       "  '0',\n",
       "  '\\\\leq',\n",
       "  '\\\\theta',\n",
       "  '\\\\leq',\n",
       "  '\\\\frac{\\\\pi\\\\sqrt{2}}{4}',\n",
       "  '']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_parsed_inputs[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fbb31b",
   "metadata": {},
   "source": [
    "### Learn tokenization with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b474a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: investigate finetuning on pretrained on custom corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4836d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: investigate different pre trained\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5d893ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Eva', '##lu', '##ate', 'the', 'limit', ':', '\\\\', '[', '\\\\', 'l', '##im', '_', '{', 'x', '\\\\', 'to', '\\\\', 'in', '##fty', '}', '\\\\', 'sq', '##rt', '{', 'x', '}', '\\\\', 'left', '(', '\\\\', 'sq', '##rt', '[', '3', ']', '{', 'x', '+', '1', '}', '-', '\\\\', 'sq', '##rt', '[', '3', ']', '{', 'x', '-', '1', '}', '\\\\', 'right', ')', '\\\\', ']']]\n",
      "[[101, 9734, 7535, 2193, 1103, 5310, 131, 165, 164, 165, 181, 4060, 168, 196, 193, 165, 1106, 165, 1107, 27944, 198, 165, 4816, 3740, 196, 193, 198, 165, 1286, 113, 165, 4816, 3740, 164, 124, 166, 196, 193, 116, 122, 198, 118, 165, 4816, 3740, 164, 124, 166, 196, 193, 118, 122, 198, 165, 1268, 114, 165, 166, 102]]\n"
     ]
    }
   ],
   "source": [
    "bert_token_ids = []\n",
    "bert_tokens = []\n",
    "for d in train_ds_qs:\n",
    "\t#tokens = tokenizer.tokenize(d)\n",
    "    bert_tokens.append(tokenizer.tokenize(d))\n",
    "    bert_token_ids.append(tokenizer.encode(d))\n",
    "\n",
    "print(bert_tokens[:1])\n",
    "print(bert_token_ids[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ceebe",
   "metadata": {},
   "source": [
    "## Perform cluster analysis on questions field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5403699d",
   "metadata": {},
   "source": [
    "## Perform cluster analysis on questions field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65175b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873aa8a3",
   "metadata": {},
   "source": [
    "## Perform document index on different fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f25adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q python-terrier==0.11.0 #now there is another version but we don't use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cc2704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/Cellar/openjdk@11/11.0.21/libexec/openjdk.jdk/Contents/Home\"\n",
    "os.environ[\"JVM_PATH\"] = \"/opt/homebrew/Cellar/openjdk@11/11.0.26/libexec/openjdk.jdk/Contents/Home/lib/server/libjvm.dylib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "377f74d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['question', 'final_answer', 'difficulty', 'topic', 'r1_solution_1', 'r1_solution_2', 'r1_solution_3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java started and loaded: pyterrier.java, pyterrier.terrier.java [version=5.11 (build: craig.macdonald 2025-01-13 21:29), helper_version=0.0.8]\n",
      "/var/folders/5w/z61drr6x4kzfb5n_v8gpvj600000gn/T/ipykernel_9509/2033746314.py:13: DeprecationWarning: Call to deprecated method pt.init(). Deprecated since version 0.11.0.\n",
      "java is now started automatically with default settings. To force initialisation early, run:\n",
      "pt.java.init() # optional, forces java initialisation\n",
      "  pt.init()\n"
     ]
    }
   ],
   "source": [
    "column_names = train_ds.column_names\n",
    "print(column_names)\n",
    "\n",
    "# 2. Initialize PyTerrier (only once)\n",
    "import pyterrier as pt\n",
    "\n",
    "# google colab\n",
    "#if not pt.started():\n",
    "#  pt.init()\n",
    "\n",
    "# vs code in mac\n",
    "if not pt.java.started():\n",
    "    pt.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f9c8f9",
   "metadata": {},
   "source": [
    "### Finding \"quadratic form\" in 'question'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d998d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5w/z61drr6x4kzfb5n_v8gpvj600000gn/T/ipykernel_9509/680686630.py:20: DeprecationWarning: Call to deprecated class BatchRetrieve. (use pt.terrier.Retriever() instead) -- Deprecated since version 0.11.0.\n",
      "  tfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  qid  docid  docno  rank     score           query\n",
      "0   1  29503  29503     0  9.515177  quadratic form\n",
      "1   1  95512  95512     1  9.180302  quadratic form\n",
      "2   1  96657  96657     2  9.049716  quadratic form\n",
      "3   1  97982  97982     3  9.049716  quadratic form\n",
      "4   1  10764  10764     4  8.987538  quadratic form\n",
      "5   1   7207   7207     5  8.978068  quadratic form\n"
     ]
    }
   ],
   "source": [
    "# 3. Build a corpus iterator over the 'question' field\n",
    "def get_question_corpus(ds):\n",
    "    for i, example in enumerate(ds):\n",
    "        yield {\n",
    "            'docno': str(i),                          # unique document ID\n",
    "            'question': example['question']           # only the question text\n",
    "        }\n",
    "\n",
    "# 4. Index into a new folder, specifying that we only want the 'question' field\n",
    "pt_index_path = './terrier_deepmath_questions'\n",
    "indexer = pt.index.IterDictIndexer(pt_index_path, overwrite=True, meta_reverse=[])\n",
    "index_ref = indexer.index(\n",
    "    get_question_corpus(train_ds),\n",
    "    fields=('question',),\n",
    "    meta=('docno', 'question')  # store docno + question in the metadata\n",
    ")\n",
    "\n",
    "# 5. Load the index and run a TF-IDF retrieval over the 'question' field\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "tfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
    "results = tfidf.search(\"quadratic form\")\n",
    "print(results.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715eff6f",
   "metadata": {},
   "source": [
    "### Perform a 'question' search in only documents with x difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28a1053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imposto difficulty = 0.0\n",
    "filtered_ds = [ex for ex in train_ds if float(ex['difficulty']) == 0.0]\n",
    "\n",
    "# Build a corpus iterator over the 'question' field\n",
    "def get_question_corpus(ds):\n",
    "    for i, example in enumerate(ds):\n",
    "        yield {\n",
    "            'docno': str(i),                          # unique document ID\n",
    "            'question': example['question']           # only the question text\n",
    "        }\n",
    "\n",
    "# Index into a new folder, specifying that we only want the 'question' field\n",
    "pt_index_path = './terrier_deepmath_questions'\n",
    "indexer = pt.index.IterDictIndexer(pt_index_path, overwrite=True, meta_reverse=[])\n",
    "index_ref = indexer.index(\n",
    "    get_question_corpus(filtered_ds),\n",
    "    fields=('question',),\n",
    "    meta=('docno', 'question')\n",
    ")\n",
    "\n",
    "# Load the index and run a TF-IDF retrieval over the 'question' field\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "tfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
    "query = \"quadratic form\"\n",
    "query1 = \"quadratic form optimization minimize\" #puoi mettere piu parole e funziona\n",
    "results = tfidf.search(query)\n",
    "print(results.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47275de",
   "metadata": {},
   "source": [
    "### Finding 'quadratic form' in more fields (NON RIUSCITO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a027d6f6",
   "metadata": {},
   "source": [
    "## Generate embeddings and analyze them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f765ad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89738007",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cae7288",
   "metadata": {},
   "source": [
    "## M1C: Base model (topic classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89af69d2",
   "metadata": {},
   "source": [
    "## M1R: Base model (difficulty regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8374fc4",
   "metadata": {},
   "source": [
    "## M1S: Base model (short answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3cd581",
   "metadata": {},
   "source": [
    "## M2: Introduce reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21808240",
   "metadata": {},
   "source": [
    "## Comparisons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
